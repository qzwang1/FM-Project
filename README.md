# FM-Metadrive

## Scenario Description

We use a **driving scenario generated by Scenic + MetaDrive on the Town01 map**.

### Environment

- Urban road network with multiple lanes and intersections  
- Lane centerlines and road boundaries are known from the map  
- The ego vehicle receives **semantic camera observations** only

### Initial State

At the beginning of each episode:

- The **ego vehicle** is spawned on a randomly selected lane
- Its **position** is sampled along the lane centerline
- Its **initial heading** has a small random perturbation relative to the lane direction (±0.25 rad)
- A **lead vehicle** is placed **8–20 meters** ahead of the ego

### Other Vehicles

In addition to the lead vehicle, we spawn several autonomous vehicles:

- They use a **DriveAvoidingCollisions** controller  
- They create:
  - background traffic
  - possible obstacles
  - pressure for lane keeping and speed control

### Objective of the Ego Vehicle

The ego agent is controlled by our learned policy.  
Its goal is to:

- **stay on the road**
- **avoid collisions**
- **maintain forward progress**

The reward includes:

- lane-keeping (cross-track error)
- orientation alignment
- speed reward
- distance between steps
- penalties for:
  - leaving the road
  - collisions

### Episode Termination Conditions

An episode terminates when:

- the ego vehicle leaves the road
- a collision occurs
- the maximum number of time steps is reached


## Training Methods

We compared three training strategies. All agents were trained with the same reinforcement learning algorithm (PPO). The only difference is **how Scenic samples scenarios** at the beginning of each episode.

### Baseline: Random Sampling

The baseline uses Scenic’s default behavior:

- Each episode randomly samples lane, position, orientation, and surrounding traffic.
- No feedback from training is used.

This provides broad coverage of the environment, but many scenes are easy and contribute little to learning.

---

### Cross-Entropy (CE) Sampler

The CE sampler uses **training feedback** to update a probability distribution over scene parameters. Sampling gradually shifts toward scenes where the agent performs poorly.

Training schedule:

- Episodes 1–50: random sampling (warm-up)
- Episodes 51+: feedback provided to CE sampler


CE focuses training on **challenging situations** that the agent has not yet mastered.

---

### Bayesian Optimization (BO) Sampler

The BO sampler treats the relationship between scene parameters and difficulty as a **black-box function**. A surrogate model (Gaussian Process) predicts which scenarios are likely to be difficult or uncertain, and BO actively proposes new parameters to explore.

This leads to **targeted exploration** of rare but critical cases.  
In our experiments, BO achieved the best overall performance.

---

### Feedback Design

Both CE and BO samplers require a single scalar feedback value.  
We define:

- feedback = -( collision + early termination + negative reward + low coverage )


Interpretation:

- More collisions, early terminations, negative reward, or low coverage ⇒ **higher difficulty**
- Since samplers **minimize feedback**, they place more sampling weight on difficult scenes.

---

### Summary

| Method | Uses Feedback? | Behavior |
|---|---|---|
| Random | No | Broad coverage, simple cases |
| CE | Yes | Concentrates on known difficult cases |
| BO | Yes | Actively searches for difficult and uncertain regions |

## Results

We trained three models (Random, CE, BO) and evaluated each for **50 test episodes** on the same set of scenes. For every episode we logged reward, collision occurrence, coverage, and episode length.

### Summary Metrics (Test Results)

| Method  | Mean Reward (↑) | Reward Std | Collision Rate (↓) | Mean Coverage (↓) | Mean Episode Length (↑) |
|--------|----------------:|-----------:|-------------------:|------------------:|------------------------:|
| **BO** | **-24.99**       | 150.79     | **0.30**           | **0.07476**       | **165.16**              |
| **CE** | -23.63          | 138.42     | 0.34               | 0.10604           | 160.72                  |
| RANDOM | -215.76         | 135.64     | **0.14**           | **0.61951**       | 113.96                  |

### Interpretation

- **Random sampling performed poorly**
  - Very low reward (large negative value)
  - Shortest episodes
  - High coverage (vehicle often far from the lane center)

- **CE improved significantly**
  - Reward increased dramatically compared to Random
  - Longer episodes
  - Lower coverage (better lane keeping)
  - But **highest collision rate**

- **BO achieved the strongest results**
  - Best episode duration
  - Best lane following (lowest coverage)
  - Good reward (close to CE)
  - Moderate collision rate

### Key Insights

- Random sampling wastes most of the training on **easy, uninformative scenes**
- CE focuses on difficult scenarios but can **overfit to specific failure cases**
- BO **actively searches for difficult and uncertain regions**, producing:
  - more diverse training scenes
  - stronger generalization

### Final Takeaway

> **BO produced the most robust driving policy**, with the best balance of reward, stability, and lane keeping.
>
> Although BO and CE have higher collision rates than Random, they drive **longer and more aggressively**, which reflects more competent behavior in challenging traffic settings.

### Future Improvements

There are several promising directions to further improve results:

- **Tune the feedback function**  
  Different weighting of reward, collision, and coverage may lead to better stability.
- **Increase training horizon**  
  More episodes (beyond 250) can give the samplers a better chance to converge.
- **Hybrid curriculum**  
  Mixing BO and CE stages (e.g. CE early, BO later) could benefit both exploration and stability.
- **Safety shaping**  
  Adding explicit penalties for lane departures or collisions may reduce failure rates while preserving aggressive driving.





